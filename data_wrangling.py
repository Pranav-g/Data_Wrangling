# -*- coding: utf-8 -*-
"""Data_Wrangling

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TTk2dJvS_LEklg2jAqPYcA9S9WRbFX61
"""

# importing python libraries

import pandas as pd
import numpy as np
from ast import literal_eval
import re
import ast
import networkx as nx

"""Mounting google drive"""

from google.colab import drive
drive.mount("/content/drive", force_remount=True)

"""# **Task 1**

## **1. Dirty Data**

### 1.1 Importing dirty data, missing data and outlier data csv along with nodes, branches, and edges data
"""

# Importing dirty data csv input
file_path = '/content/drive/Shared drives/FIT5196_S1_2024/A2/student_data/Group016_dirty_data.csv'

# Read the CSV file into a DataFrame
df = pd.read_csv(file_path)

#importing missing data csv
file_path = '/content/drive/Shared drives/FIT5196_S1_2024/A2/student_data/Group016_missing_data.csv'

# Read the CSV file into a DataFrame
data = pd.read_csv(file_path)

#importing missing data csv
file_path = '/content/drive/Shared drives/FIT5196_S1_2024/A2/student_data/Group016_outlier_data.csv'

# Read the CSV file into a DataFrame
data_out = pd.read_csv(file_path)

# Read Braches, Nodes and Edges
file_path = '/content/drive/Shared drives/FIT5196_S1_2024/A2/supplementary_data/nodes.csv'
nodes_df = pd.read_csv(file_path)
file_path = '/content/drive/Shared drives/FIT5196_S1_2024/A2/supplementary_data/branches.csv'
branches_df = pd.read_csv(file_path)
file_path = '/content/drive/Shared drives/FIT5196_S1_2024/A2/supplementary_data/edges.csv'
edges_df = pd.read_csv(file_path)
# print(branches_data.head())

# print(df_md.head())
# print(df.head())

"""### 1.2. This code block fixes the date column to YYYY-MM-DD format. Some of the dates are not in the right format. To fix this, we have checked the values, moved them to make the date more sensible"""

def process_data(data):
    parts = data.split('-')
    for i, part in enumerate(parts):
        if len(part) == 4:  # If a part has 4 digits, move it to the beginning
            parts.insert(0, parts.pop(i))
            break  # Exit loop after moving the 4-digit part to the beginning
    if len(parts) == 3 and int(parts[1]) > 12:  # If the second part is greater than 12, swap it with the third part
        parts[1], parts[2] = parts[2], parts[1]
    return '-'.join(parts)

# Apply the function to the data column
df['date'] = df['date'].apply(process_data)
# df['date'] = pd.to_datetime(df['date'])
df['date']
df.head(38)

"""### 1.3. This code block fixes the breakfast, lunch and dinner. They were incorrect with respect to time order was made."""

# Define function to correct order_type based on time
def correct_order_type(row):
    time = row['time']
    order_type = row['order_type']
    if order_type == 'Breakfast' and not ('08:00:00' <= time <= '12:00:00'):
        return 'Lunch' if '12:00:01' <= time <= '16:00:00' else 'Dinner'
    elif order_type == 'Lunch' and not ('12:00:01' <= time <= '16:00:00'):
        return 'Dinner' if '16:00:01' <= time <= '20:00:00' else 'Breakfast'
    elif order_type == 'Dinner' and not ('16:00:01' <= time <= '20:00:00'):
        return 'Breakfast' if '20:00:01' <= time or time < '08:00:00' else 'Lunch'
    else:
        return order_type

# Apply the function to correct the order_type column
df['order_type'] = df.apply(correct_order_type, axis=1)

df.head(28)

"""### 1.4 To check the error in branch code of every order_id, first lets check if their are any common branch code across the three sheets"""

# Find common order_ids between dirty_data and missing_data
common_dirty_missing = df[df['order_id'].isin(data['order_id'])]

# Find common order_ids between dirty_data and outlier_data
common_dirty_outlier = df[df['order_id'].isin(data_out['order_id'])]

# Find common order_ids between missing_data and outlier_data
common_missing_outlier = data[data['order_id'].isin(data_out['order_id'])]

# Print the results
print("Common order_ids between Dirty Data and Missing Data:")
print(common_dirty_missing['order_id'])

print("\nCommon order_ids between Dirty Data and Outlier Data:")
print(common_dirty_outlier['order_id'])

print("\nCommon order_ids between Missing Data and Outlier Data:")
print(common_missing_outlier['order_id'])

"""### 1.4.2 Since every csv sheet contains unique order_id, the only error in the branch code seems to be lowercase brach codes, fixing them."""

df['branch_code'] = df['branch_code'].str.upper()
df.head(22)

"""### 1.5 This part of the code understands which branch is serving what items as breakfast lunch and dinner since they are in different management"""

import pandas as pd
from ast import literal_eval

# Combine the data from missing and outlier datasets
combined_data = pd.concat([data, data_out])

# Helper function to parse order items

def parse_order_items(item_string):
    try:
        return literal_eval(item_string)
    except (ValueError, SyntaxError):
        return []

# Expand the order items into rows
combined_data['order_items'] = combined_data['order_items'].apply(parse_order_items)
expanded_orders = combined_data.explode('order_items')

# Split the tuple into separate columns for item name and count
expanded_orders[['item_name', 'item_count']] = pd.DataFrame(expanded_orders['order_items'].tolist(), index=expanded_orders.index)

# Group by branch code and order type and get unique item names
unique_items = expanded_orders.groupby(['branch_code', 'order_type'])['item_name'].unique().reset_index()

# Convert the array of unique items into a more readable list format
unique_items['item_name'] = unique_items['item_name'].apply(lambda x: list(x))

print("Unique items served per branch and meal type:")
print(unique_items)

"""### 1.5.1 Now we will check using the branch code and time of the day, to verify the items ordered, since there is no real way to know what was ordered in place of that item, we will just drop the item from the meal."""

# Helper function to parse and filter order items based on typical menu
def filter_order_items(row, menu_data):
    # Find the typical items for the branch and meal type
    typical_items = menu_data[(menu_data['branch_code'] == row['branch_code']) & (menu_data['order_type'] == row['order_type'])]['item_name'].values[0]
    # Parse the current order items
    current_items = literal_eval(row['order_items'])
    # Filter items that are not on the typical menu
    filtered_items = [item for item in current_items if item[0] in typical_items]
    # Check if any item was removed
    if len(filtered_items) != len(current_items):
        # Return modified items and flag this row as changed
        return str(filtered_items), True
    else:
        # Return original items and flag as unchanged
        return row['order_items'], False

# Apply the filter function to the dirty data DataFrame
results = df.apply(filter_order_items, args=(unique_items,), axis=1)
df['order_items'] = results.apply(lambda x: x[0])
df['modified'] = results.apply(lambda x: x[1])

# Filter to get the order_ids of modified orders
modified_orders = df[df['modified']]['order_id']

print("Order IDs of modified orders:")
print(modified_orders)

# df.head(19)

"""### 1.6 sklearn linera algebra have been used here to calculate price of each item using the missing data.csv. Stored those values and solve the equations in dirty_data.csv from those values from missing data file"""

# Function to parse order_items from string to list of tuples
def parse_order_items(row):
    try:
        # Using literal_eval to safely evaluate the string
        return literal_eval(row)
    except Exception as e:
        # Print the error and return the row unchanged if parsing fails
        print(f"Error parsing row: {row}, Error: {e}")
        return row

# Apply the function to the 'order_items' column
data['order_items'] = data['order_items'].apply(parse_order_items)

# Display the first few entries to verify successful parsing
print(data['order_items'].head(10))

import numpy as np
import pandas as pd



# Collect all unique items
item_set = set()
for items in data['order_items']:
    for item, quantity in items:
        item_set.add(item)

item_list = sorted(list(item_set))
item_index = {item: idx for idx, item in enumerate(item_list)}

# Initialize matrix A and vector b
A = []
b = []

for items, total_price in zip(data['order_items'], data['order_price']):
    # Create a row for matrix A with all zeros
    row = [0] * len(item_list)
    for item, quantity in items:
        row[item_index[item]] = quantity
    A.append(row)
    b.append(total_price)

A = np.array(A)
b = np.array(b)

# Use numpy.linalg.lstsq to find the least squares solution
prices, residuals, rank, s = np.linalg.lstsq(A, b, rcond=None)

# Map the prices back to items
item_prices = dict(zip(item_list, prices))

for item, price in item_prices.items():
    print(f"{item}: ${price:.2f}")

"""### 1.6.1 This code block fixes the incorrect order value. Please note that incorrect dishes which were not supposed to be present at that of the day at that branch have also been removed"""

df['order_items'] = df['order_items'].apply(literal_eval)

def calculate_correct_order_price(order_items, item_prices):
    total_price = 0
    for item, quantity in order_items:
        if item in item_prices:
            total_price += item_prices[item] * quantity
        else:
            print(f"Warning: Price for item '{item}' not found. It will not be included in the total.")
    return total_price

# Apply the function to calculate correct order prices
df['correct_order_price'] = df['order_items'].apply(lambda x: calculate_correct_order_price(x, item_prices))

# Review differences between original and correct prices
df['price_difference'] = df['order_price'] - df['correct_order_price']
print(df[['order_price', 'correct_order_price', 'price_difference']].head())

# Optionally, update the original order_price to the correct order_price
df['order_price'] = df['correct_order_price']

df.head(19)

"""### 1.7 Its been observed that some of the longitudes and lattitudes have been interchanged within, this code changes them back and make the data more reliabale"""

# Define reasonable ranges for latitude and longitude
lat_range = (-90, 90)
lon_range = (-180, 180)

# Select rows with incorrect latitude or longitude values
incorrect_coords = df[
    (df['customer_lat'] < lat_range[0]) | (df['customer_lat'] > lat_range[1]) |
    (df['customer_lon'] < lon_range[0]) | (df['customer_lon'] > lon_range[1])
]

# Swap latitude and longitude values
df.loc[incorrect_coords.index, ['customer_lat', 'customer_lon']] = df.loc[
    incorrect_coords.index, ['customer_lon', 'customer_lat']
].values

# Display the rows with swapped coordinates along with the order_id
print(df.loc[incorrect_coords.index, ['order_id', 'customer_lat', 'customer_lon']])

df.head(51)

"""### 1.8 This part of the column trains a model to predict delievry fee and and then check the customer loyality value, if the predicted price is half of actual price, then the customer loyality should have been 1 instead of zero and vice versa"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Preprocess the data
# Convert 'date' and 'time' into a single datetime column (if needed)
df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'], errors='coerce')

# Handling any missing values or errors
df.dropna(inplace=True)  # Example of dropping rows with missing values

# Select features and target
X = df[['order_price', 'distance_to_customer_KM', 'order_type', 'branch_code']]
y = df['delivery_fee']

# Encoding categorical data
categorical_features = ['order_type', 'branch_code']
numeric_features = ['order_price', 'distance_to_customer_KM']

preprocessor = ColumnTransformer(
    transformers=[
        ('num', 'passthrough', numeric_features),
        ('cat', OneHotEncoder(), categorical_features)
    ])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Save test indices
test_indices = X_test.index

# Create and train the model
model = Pipeline(steps=[('preprocessor', preprocessor),
                        ('regressor', LinearRegression())])

model.fit(X_train, y_train)

# Predicting the delivery fees
y_pred = model.predict(X_test)

# Implement the loyalty logic
def loyalty_score(actual, predicted):
    ratio = predicted / actual
    if 0.5 <= ratio <= 1.5:
        return 1  # High loyalty
    else:
        return 0  # Low loyalty

# Apply the loyalty function to the test set
loyalty_scores = [loyalty_score(act, pred) for act, pred in zip(y_test, y_pred)]
df.loc[test_indices, 'customerHasloyalty?'] = loyalty_scores  # Update only for test set indices




df.head(30)

"""### 1.9 This part of the code calculates distance between branch and cosutomer using dijkstras algorithm and changes the value if algorithm found some shortest path"""

customer_lat_col = 'customer_lat'
customer_lon_col = 'customer_lon'
branch_lat_col = 'branch_lat'
branch_lon_col = 'branch_lon'
branch_code_col = 'branch_code'

# Create a graph from the edges data
G = nx.Graph()
for index, row in edges_df.iterrows():
    G.add_edge(row['u'], row['v'], weight=row['distance(m)'])

# Function to find the nearest node for given coordinates
def find_nearest_node(lat, lon):
    nodes_df['distance'] = np.sqrt((nodes_df['lat'] - lat) ** 2 + (nodes_df['lon'] - lon) ** 2)
    nearest_node = nodes_df.loc[nodes_df['distance'].idxmin()]['node']
    return nearest_node

# Calculate the shortest distance between customer and branch
def calculate_shortest_distance(customer_lat, customer_lon, branch_lat, branch_lon):
    customer_node = find_nearest_node(customer_lat, customer_lon)
    branch_node = find_nearest_node(branch_lat, branch_lon)
    shortest_path_length = nx.dijkstra_path_length(G, source=customer_node, target=branch_node)
    return shortest_path_length

# Add branch coordinates to dirty_data_df
df = df.merge(branches_df, on=branch_code_col, how='left')

# Fill NaN values for coordinates
df[customer_lat_col].fillna(0, inplace=True)
df[customer_lon_col].fillna(0, inplace=True)
df[branch_lat_col].fillna(0, inplace=True)
df[branch_lon_col].fillna(0, inplace=True)

# Calculate the shortest distance for each order
shortest_distances = []
for index, row in df.iterrows():
    if pd.notna(row[customer_lat_col]) and pd.notna(row[customer_lon_col]) and pd.notna(row[branch_lat_col]) and pd.notna(row[branch_lon_col]):
        distance = calculate_shortest_distance(row[customer_lat_col], row[customer_lon_col], row[branch_lat_col], row[branch_lon_col])
        shortest_distances.append(distance)
    else:
        shortest_distances.append(np.nan)

df['distance_to_customer_KM'] = shortest_distances
df['distance_to_customer_KM'] = df['distance_to_customer_KM']/1000

# Drop the temporary columns if they are not needed further
df.drop(columns=['modified','correct_order_price',	'price_difference',	'branch_name'	,'branch_lat'	,'branch_lon'], inplace=True)

# Drop the 'datetime' column as it's no longer needed
df.drop(columns=['datetime'], inplace=True)

# Save the modified DataFrame to a CSV file
output_file_path = '/content/drive/My Drive/016_dirty_data_solution.csv'
df.to_csv(output_file_path, index=False)

output_file_path

print(len(df))

df.head(10)

"""## **2. Imputing Missing data**

### The Approach taken to impute missing fee is
1) First we tried removing the outlier values from outlier_csv then merging it with missing_data csv and then another approach was merging both of them first and then removing the outlier, the second appraoch was yeiling better R^2 values so we have used that

2) we have made a copy of missingfile data as while merging the model, it was giving the NaN value error, so we deleted the columns with NaN values after making the copy. Now out data is ready for further processing

3) We have used one-hot encoding to use branch code in our model training

4) As mentioned in scenerio, there was 2 linear relationship was given for time of the day and if the date was weekend or not. Along with this we have also incorporated the customer loyality column as it would have messed with the trained model. The logical approach would have been to noramlise this column but dur to time constrain, we chose to use it as 'if' condition

5) Then we trained out model, imputed the missing value in copied dataframe and took everything out in an csv
"""

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder



original_data = data.copy()  # Make a copy of the original data to retain all rows

# Drop rows with missing 'branch_code' and 'distance_to_customer_KM' for training
data_clean = data.dropna(subset=['branch_code', 'distance_to_customer_KM'])
data_out_clean = data_out.dropna(subset=['branch_code', 'distance_to_customer_KM'])

# Concatenate both data frames
combined_data = pd.concat([data_clean, data_out_clean])

# Filtering numeric columns for outlier detection
numeric_cols = combined_data.select_dtypes(include=[np.number]).columns
combined_data_numeric = combined_data[numeric_cols]

# Outlier removal using IQR on numeric columns
Q1 = combined_data_numeric.quantile(0.25)
Q3 = combined_data_numeric.quantile(0.75)
IQR = Q3 - Q1
is_outlier = (combined_data_numeric < (Q1 - 1.5 * IQR)) | (combined_data_numeric > (Q3 + 1.5 * IQR))
filtered_data = combined_data[~is_outlier.any(axis=1)]

# Encoding 'branch_code' using one-hot encoding
encoder = OneHotEncoder(drop='first', sparse=False)
branch_codes_encoded = encoder.fit_transform(filtered_data[['branch_code']])
branch_code_cols = encoder.get_feature_names_out(['branch_code'])
encoded_df = pd.DataFrame(branch_codes_encoded, columns=branch_code_cols, index=filtered_data.index)

# Concatenate the encoded DataFrame with the filtered data
filtered_data = pd.concat([filtered_data, encoded_df], axis=1)

# Function to map time to categorical
def map_time_to_categorical(time_str):
    hour = pd.to_datetime(time_str).hour
    if 8 <= hour < 12:
        return 0  # Morning
    elif 12 <= hour < 16:
        return 1  # Afternoon
    else:
        return 2  # Evening

filtered_data['time_of_day'] = filtered_data['time'].apply(map_time_to_categorical)
filtered_data['is_weekend'] = pd.to_datetime(filtered_data['date']).dt.dayofweek.isin([5, 6]).astype(int)
filtered_data['delivery_fee'] = np.where(filtered_data['customerHasloyalty?'] == 1, filtered_data['delivery_fee'] / 2, filtered_data['delivery_fee'])

# Ensure all columns are included and non-null before training
feature_cols = ['is_weekend', 'time_of_day', 'distance_to_customer_KM'] + list(branch_code_cols)
filtered_data = filtered_data.dropna(subset=feature_cols + ['delivery_fee'])

# Train a linear regression model
X = filtered_data[feature_cols]
y = filtered_data['delivery_fee']

model = LinearRegression()
model.fit(X, y)

# Predict delivery fee
filtered_data['predicted_delivery_fee'] = model.predict(X)

# Evaluate the model
rmse = np.sqrt(mean_squared_error(y, model.predict(X)))
r2 = r2_score(y, model.predict(X))

print(f"RMSE: {rmse}, R²: {r2}")

# Predict delivery fee for original dataset where needed
rows_to_predict = original_data[original_data['delivery_fee'].isnull() &
                                original_data['branch_code'].notnull() &
                                original_data['distance_to_customer_KM'].notnull()]

if not rows_to_predict.empty:
    encoded_branch = encoder.transform(rows_to_predict[['branch_code']])
    rows_to_predict = pd.concat([rows_to_predict.drop(columns=['branch_code']), pd.DataFrame(encoded_branch, columns=branch_code_cols, index=rows_to_predict.index)], axis=1)
    rows_to_predict['time_of_day'] = rows_to_predict['time'].apply(map_time_to_categorical)
    rows_to_predict['is_weekend'] = pd.to_datetime(rows_to_predict['date']).dt.dayofweek.isin([5, 6]).astype(int)

    X_to_predict = rows_to_predict[feature_cols]
    predicted_fees = model.predict(X_to_predict)

    # Copy the predicted delivery fees into the original 'delivery_fee' column where it was missing
    original_data.loc[rows_to_predict.index, 'delivery_fee'] = predicted_fees

    print("Predicted delivery fees added to the original data.")
print(predicted_fees)
# Print the head of the DataFrame to see the updated dataset with predicted fees next to the original fees
original_data.head(10)
print(len(original_data))


# Save the modified DataFrame to a CSV file
output_file_path = '/content/drive/My Drive/016_missing_data_solution.csv'
df.to_csv(output_file_path, index=False)

"""## **3. Handling Outliers**

3.1 Calculating coeficients, generating equations and models and developing boxplot graph branch wise, to visualise outlier data per branch
"""

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
def load_data(filepath):
    return pd.read_csv(filepath)

# Preprocess data
def preprocess_data(data):
    data['date'] = pd.to_datetime(data['date'])
    data['is_weekend'] = data['date'].dt.dayofweek.apply(lambda x: 1 if x >= 5 else 0)
    data['time_of_day'] = data['time'].apply(lambda x: classify_time(x))
    # Adjust the delivery fee for loyalty customers
    data['adjusted_delivery_fee'] = data.apply(lambda row: row['delivery_fee'] / 0.5 if row['customerHasloyalty?'] == 1 else row['delivery_fee'], axis=1)
    return data

# Classify time of day
def classify_time(time_str):
    time = pd.Timestamp(time_str)
    if time.hour < 12:
        return 0  # Morning
    elif 12 <= time.hour < 17:
        return 1  # Afternoon
    else:
        return 2  # Evening

# Build and evaluate model for each branch
def build_model(data, branch_code):
    data_branch = data[data['branch_code'] == branch_code]
    X = data_branch[['is_weekend', 'time_of_day', 'distance_to_customer_KM']]
    y = data_branch['adjusted_delivery_fee']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    model = LinearRegression()
    model.fit(X_train, y_train)
    return model.coef_, model.intercept_, mean_squared_error(y_test, model.predict(X_test), squared=False)

# Function to plot boxplots
def plot_boxplots(data):
    plt.figure(figsize=(10, 6))
    sns.boxplot(x='branch_code', y='adjusted_delivery_fee', data=data)
    plt.title('Boxplot of Adjusted Delivery Fees by Branch')
    plt.xlabel('Branch Code')
    plt.ylabel('Adjusted Delivery Fee')
    plt.show()

# Main function
def main(filepath):
    data = load_data(filepath)
    data = preprocess_data(data)
    branches = data['branch_code'].unique()
    results = {}

    for branch in branches:
        coefficients, intercept, rmse = build_model(data, branch)
        results[branch] = {
            'Equation': f"Delivery Fee = {intercept:.2f} + {coefficients[0]:.2f} * Weekend + {coefficients[1]:.2f} * Time of Day + {coefficients[2]:.2f} * Distance",
            'RMSE': rmse
        }
        print(f"Branch {branch} Model:")
        print(results[branch]['Equation'])
        print(f"  RMSE: {results[branch]['RMSE']:.2f}\n")

    plot_boxplots(data)
    return results

# Run the analysis
if __name__ == "__main__":
    file_path = '/content/drive/Shared drives/FIT5196_S1_2024/A2/student_data/Group016_outlier_data.csv'
    main(file_path)

"""3.2 pointing out the outliers which do not satisfy this analysis. By analysys we meant whichever outlier donot fit the equation we got for the three branches, we have deleted them. before deleting them we have also considered the customer loyality value as it is obviosu that if customer loyality is '1', the amouth of fee chaged would be different. and then saved everything to csv"""

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
def load_data(filepath):
    data = pd.read_csv(filepath)
    data['date'] = pd.to_datetime(data['date'])
    data['is_weekend'] = data['date'].dt.dayofweek.apply(lambda x: 1 if x >= 5 else 0)
    data['time_of_day'] = data['time'].apply(lambda x: 0 if pd.to_datetime(x).hour < 12 else (1 if pd.to_datetime(x).hour < 17 else 2))
    data['adjusted_delivery_fee'] = data.apply(lambda row: row['delivery_fee'] / 0.5 if row['customerHasloyalty?'] == 1 else row['delivery_fee'], axis=1)
    return data

# Function to calculate residuals
def calculate_residuals(model, X, y):
    predictions = model.predict(X)
    residuals = y - predictions
    return residuals

# Function to identify and remove outliers based on residuals
def remove_outliers(data, branch_code, threshold=1.5):
    data_branch = data[(data['branch_code'] == branch_code) & (data['customerHasloyalty?'] == 0)]
    X = data_branch[['is_weekend', 'time_of_day', 'distance_to_customer_KM']]
    y = data_branch['adjusted_delivery_fee']

    model = LinearRegression()
    model.fit(X, y)
    residuals = calculate_residuals(model, X, y)
    data_branch['residuals'] = residuals

    outlier_condition = np.abs(residuals) > (np.std(residuals) * threshold)
    clean_data = data_branch[~outlier_condition]
    return clean_data

# Function to plot residuals distribution
def plot_residuals(data, branch_code):
    X = data[['is_weekend', 'time_of_day', 'distance_to_customer_KM']]
    y = data['adjusted_delivery_fee']

    model = LinearRegression()
    model.fit(X, y)
    residuals = calculate_residuals(model, X, y)

    plt.figure(figsize=(10, 6))
    sns.histplot(residuals, kde=True)
    plt.title(f'Residuals Distribution for Branch {branch_code}')
    plt.xlabel('Residuals')
    plt.ylabel('Frequency')
    plt.show()

# Main function to execute analysis
def main():
    filepath = '/content/drive/Shared drives/FIT5196_S1_2024/A2/student_data/Group016_outlier_data.csv'
    data = load_data(filepath)

    branches = data['branch_code'].unique()
    clean_frames = []  # List to store cleaned dataframes for each branch

    for branch in branches:
        print(f"Analyzing branch {branch}")
        clean_data = remove_outliers(data, branch)
        clean_frames.append(clean_data)
        print(f"Cleaned data for branch {branch}:")
        print(clean_data[['order_id', 'is_weekend', 'time_of_day', 'distance_to_customer_KM', 'adjusted_delivery_fee', 'residuals']])
        plot_residuals(clean_data, branch)

    # Concatenate all cleaned dataframes
    final_cleaned_data = pd.concat(clean_frames)
    # Save to CSV
    final_cleaned_data.to_csv('/content/drive/My Drive/016_outlier_data_solution.csv', index=False)
    print("Cleaned data saved to '/path/to/save/cleaned_data.csv'.")

if __name__ == "__main__":
    main()

"""# **Task 2**

## 2.1 Starting the analysis by doing simple EDA
"""

import pandas as pd



# Select the columns of interest
columns_of_interest = ['order_price', 'distance_to_customer_KM', 'delivery_fee']

# Get the descriptive statistics for the selected numerical columns
descriptive_stats = data_out[columns_of_interest].describe()

# Print the statistics
print("Descriptive Statistics for Selected Columns:")
print(descriptive_stats)

"""It has been observed that mean and standard deviation id very off for allthe three columns

### 2.2 Extracting grphs for all the column, even for branch code using one-hot encoding
"""

import pandas as pd
import matplotlib.pyplot as plt



# Perform one-hot encoding on 'branch_code'
data_encoded = pd.get_dummies(data_out, columns=['branch_code'])

# Convert boolean columns to integer (if any)
boolean_columns = data_encoded.select_dtypes(include=['bool']).columns
data_encoded[boolean_columns] = data_encoded[boolean_columns].astype(int)

# Set up the figure size for a large grid of histograms
plt.figure(figsize=(20, 15))

# List all columns for histograms including the newly encoded ones
all_columns = data_encoded.columns.drop(['order_id', 'date', 'time', 'order_items'])  # Excluding non-numeric or non-relevant columns

# Calculate the number of rows needed in the subplot grid
num_rows = len(all_columns) // 3 + (len(all_columns) % 3 > 0)

# Generate histograms for each column
for i, column in enumerate(all_columns):
    plt.subplot(num_rows, 3, i + 1)  # Dynamically adjust grid size
    plt.hist(data_encoded[column].dropna(), bins=20, color='skyblue', edgecolor='black')  # Drop NA for safety
    plt.title(f'Histogram of {column}')
    plt.xlabel(column)
    plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

"""### 3.3 Its been observed that data is skewed for order_price, distance and fee. To reduce the skewness we have used logarithmic transformation"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


# Apply logarithmic transformations for 'order_price', 'distance_to_customer_KM', and 'delivery_fee'
data_out['log_order_price'] = np.where(data_out['order_price'] > 0, np.log(data_out['order_price']), None)
data_out['log_distance_to_customer_KM'] = np.where(data_out['distance_to_customer_KM'] > 0, np.log(data_out['distance_to_customer_KM']), None)
data_out['log_delivery_fee'] = np.where(data_out['delivery_fee'] > 0, np.log(data_out['delivery_fee']), None)

# Set up the figure for plotting
plt.figure(figsize=(15, 5))

# Scatter plot for Log Order Price vs. Log Delivery Fee
plt.subplot(1, 3, 1)
sns.scatterplot(x=data_out['log_order_price'], y=data_out['log_delivery_fee'])
plt.title('Log Order Price vs. Log Delivery Fee')
plt.xlabel('Log Order Price')
plt.ylabel('Log Delivery Fee')

# Scatter plot for Log Distance to Customer KM vs. Log Delivery Fee
plt.subplot(1, 3, 2)
sns.scatterplot(x=data_out['log_distance_to_customer_KM'], y=data_out['log_delivery_fee'])
plt.title('Log Distance to Customer KM vs. Log Delivery Fee')
plt.xlabel('Log Distance to Customer KM')
plt.ylabel('Log Delivery Fee')

# Scatter plot for Log Order Price vs. Log Distance to Customer KM
plt.subplot(1, 3, 3)
sns.scatterplot(x=data_out['log_order_price'], y=data_out['log_distance_to_customer_KM'])
plt.title('Log Order Price vs. Log Distance to Customer KM')
plt.xlabel('Log Order Price')
plt.ylabel('Log Distance to Customer KM')

plt.tight_layout()
plt.show()

"""### 3.4 After using logarithmic transformation and obtaining graph of its been observed that there are too many vertical peaks, to reduce them, we have minmax transformation from sklear library"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler



# Apply logarithmic transformations where applicable
data_out['log_order_price'] = np.where(data_out['order_price'] > 0, np.log(data_out['order_price']), None)
data_out['log_distance_to_customer_KM'] = np.where(data_out['distance_to_customer_KM'] > 0, np.log(data_out['distance_to_customer_KM']), None)
data_out['log_delivery_fee'] = np.where(data_out['delivery_fee'] > 0, np.log(data_out['delivery_fee']), None)

# Perform one-hot encoding on 'branch_code'
data_encoded = pd.get_dummies(data_out, columns=['branch_code'])

# Select columns to scale
columns_to_scale = ['log_order_price', 'log_distance_to_customer_KM', 'log_delivery_fee', 'customerHasloyalty?'] + \
                    [col for col in data_encoded.columns if col.startswith('branch_code')]

# Initialize Min-Max Scaler
scaler = MinMaxScaler()

# Fit and transform the data
data_encoded[columns_to_scale] = scaler.fit_transform(data_encoded[columns_to_scale].fillna(0))  # Handling None by filling with 0

# Display the first few rows of the scaled data to verify
print(data_encoded[columns_to_scale].head())

"""### Finally plotting the graph with reduced skewness and peaks"""

import matplotlib.pyplot as plt
import seaborn as sns

# Set up the figure size for a large grid of histograms/density plots
plt.figure(figsize=(18, 12))

# List all columns that were scaled
columns_to_plot = ['log_order_price', 'log_distance_to_customer_KM', 'log_delivery_fee', 'customerHasloyalty?'] + \
                  [col for col in data_encoded.columns if col.startswith('branch_code')]

# Generate density plots for each scaled column
for i, column in enumerate(columns_to_plot):
    plt.subplot(3, 4, i + 1)  # Adjust grid size according to number of columns
    sns.histplot(data_encoded[column], kde=True, element='step', fill=False, color='blue', stat='density')
    plt.title(f'Density Plot of {column}')

plt.tight_layout()
plt.show()

